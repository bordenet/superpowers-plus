---
name: perplexity-research
description: "Invoke when stuck (2+ failed attempts, uncertainty, or guessing) OR manually to research technical/domain questions via Perplexity MCP. ALWAYS announce invocation and track stats."
---

# Perplexity Research

> **Purpose**: Get unstuck by dispatching research queries to Perplexity AI
> **Trigger**: Automatic (2+ failures, uncertainty) OR manual invocation
> **Stats**: `~/.codex/perplexity-stats.json`

## When to Invoke (Automatic Triggers)

You MUST invoke this skill when ANY of these conditions are met:

| Trigger | Description |
|---------|-------------|
| **2+ Failed Attempts** | Same operation failed twice with different approaches |
| **Uncertainty/Guessing** | You're unsure and would otherwise guess at an answer |
| **Cutting Corners** | About to do something that violates Agents.md guidance |
| **Hallucination Risk** | Making claims about APIs, libraries, or facts you're not certain about |
| **Outdated Knowledge** | Question involves tools/frameworks potentially released after training |
| **Unknown Errors** | Encountering error messages you can't interpret |

**Personal preference matters?** â†’ Ask the user instead
**Broader/extrinsic research needed?** â†’ Invoke this skill

## Manual Invocation

User can always force invocation with:
- "Use Perplexity to research X"
- "Get unstuck on X"  
- "Research X via Perplexity"

## Invocation Protocol

### Step 0: Try Free Tools First (MANDATORY)

<EXTREMELY_IMPORTANT>
**Before ANY Perplexity call, you MUST try free alternatives first.**

1. **Use `web-search`** for the query
2. **Use `web-fetch`** to read any promising URLs
3. **Evaluate**: Did you find what you need? If YES â†’ STOP (no Perplexity needed)
4. **Only escalate** if results are â‰¥50% worse than expected

**You MUST state explicitly before calling Perplexity:**
> "web-search returned [X]. This is insufficient because [reason]. Escalating to Perplexity."

**Never use Perplexity for:**
- Simple company lookups
- Checking if a URL is live
- Finding a company's website
- Basic fact-checking
- Reading documentation pages
</EXTREMELY_IMPORTANT>

### Step 1: Announce

**ALWAYS** announce before invoking:

```
ðŸ” **Consulting Perplexity**: [Brief description of what I'm researching]
Reason: [Which trigger condition was met]
Free tools tried: [web-search result summary]
Why escalating: [specific insufficiency]
```

### Step 2: Generate Rich Prompt

Craft a detailed prompt for Perplexity that includes:
- **Context**: What you're trying to accomplish
- **Specific question**: The exact information needed
- **Constraints**: Any requirements (language, version, platform)
- **What you've tried**: Failed approaches (if applicable)
- **What web-search found**: Summary of free tool results

### Step 3: Dispatch to Perplexity

Use the Perplexity MCP tools:

| Query Type | Tool |
|------------|------|
| Quick fact | `perplexity_search_perplexity` |
| How-to question | `perplexity_ask_perplexity` |
| Deep research | `perplexity_research_perplexity` |
| Complex reasoning | `perplexity_reason_perplexity` |

### Step 4: Report Results (Preliminary)

After receiving response, report what you learned:

```
ðŸ“‹ **Perplexity Response**: [Summary of findings]

Key insights:
- [insight 1]
- [insight 2]

**Attempting to apply**: [specific action you'll take based on this]
```

### Step 5: Apply the Information

Actually USE the information from Perplexity:
- Run the suggested command
- Implement the suggested fix
- Apply the recommended approach
- Test the solution

**DO NOT record stats yet.** You must evaluate whether it helped first.

### Step 6: Evaluate Helpfulness (CRITICAL)

After attempting to apply the Perplexity response, explicitly evaluate:

```
ðŸ“Š **Perplexity Evaluation**:
- Applied: [what you tried]
- Outcome: [SUCCESS | PARTIAL | FAILURE]
- Reason: [why it helped or didn't help]
```

**Evaluation criteria**:

| Outcome | Criteria | Record As |
|---------|----------|-----------|
| **SUCCESS** | Problem solved, unblocked, or gained actionable insight | `successful: true` |
| **PARTIAL** | Some useful info but needed additional work | `successful: true` |
| **FAILURE** | Information was wrong, irrelevant, or didn't help | `successful: false` |

### Step 7: Update Stats (After Evaluation)

**ONLY after Step 6**, update `~/.codex/perplexity-stats.json`:

```json
{
  "total_invocations": 42,
  "successful": 38,
  "unsuccessful": 4,
  "success_rate": 0.905,
  "last_invocation": "2026-01-31T10:30:00Z",
  "by_trigger": {
    "failed_attempts": 15,
    "uncertainty": 12,
    "outdated_knowledge": 8,
    "unknown_errors": 5,
    "manual": 2
  },
  "by_tool": {
    "search": 10,
    "ask": 20,
    "research": 10,
    "reason": 2
  },
  "recent": [
    {
      "timestamp": "2026-01-31T10:30:00Z",
      "trigger": "failed_attempts",
      "tool": "ask",
      "query_summary": "ESLint 9.x flat config ignore patterns",
      "successful": true,
      "outcome": "SUCCESS",
      "outcome_reason": "Fixed the ignore pattern issue"
    }
  ]
}
```

**The evaluation loop**:
1. Receive Perplexity response â†’ Report (Step 4)
2. Apply the information â†’ Act (Step 5)
3. Evaluate outcome â†’ Judge (Step 6)
4. Record stats â†’ Track (Step 7)

## Stats Commands

View stats: `cat ~/.codex/perplexity-stats.json | jq .`

Reset stats: `echo '{"total_invocations":0,"successful":0,"unsuccessful":0,"success_rate":0,"by_trigger":{},"by_tool":{},"recent":[]}' > ~/.codex/perplexity-stats.json`

## Integration with Other Skills

- **superpowers:systematic-debugging**: Invoke Perplexity when debugging hits a wall
- **superpowers:brainstorming**: Use for research during design exploration
- **superpowers:verification-before-completion**: Verify facts before claiming done

## Cost Efficiency (CRITICAL)

> âš ï¸ **Perplexity API calls are NOT free.** Use efficiently but DO use when confident it will help.

### High-Value Use Cases (DO use Perplexity)

| Use Case | Why It's Worth It |
|----------|-------------------|
| **Deep research before major feature work** | Prevents costly rework |
| **PRD/design review and refinement** | High-value feedback on architecture |
| **State-of-the-art research in specialized domains** | Training data may be outdated |
| **Validating architectural decisions** | Industry standards evolve |
| **Complex debugging after 2+ failures** | Time saved > API cost |

### Low-Value Use Cases (Use alternatives instead)

| Use Case | Better Alternative |
|----------|-------------------|
| Simple factual lookups | `web-search` tool |
| Code examples | `codebase-retrieval` or documentation |
| General knowledge questions | Rely on training data |
| Iterative refinement | Batch questions into single call |

### Efficiency Tactics

1. **Batch related questions** - Combine multiple questions into one call
2. **Use `strip_thinking: true`** - Reduces token usage for research/reason tools
3. **Choose the right tool**:
   - `perplexity_search_perplexity` - Quick facts (cheapest)
   - `perplexity_ask_perplexity` - How-to questions (moderate)
   - `perplexity_research_perplexity` - Deep dives (expensive)
   - `perplexity_reason_perplexity` - Complex reasoning (most expensive)
4. **Fallback to web-search** - When Perplexity is unavailable or for simple lookups

### Decision Framework (Cost-Conscious)

```
Step 1: Try web-search first (FREE)
â”œâ”€â”€ Found what I need? â†’ STOP (no Perplexity)
â””â”€â”€ Insufficient? â†’ Continue

Step 2: Try web-fetch on promising URLs (FREE)
â”œâ”€â”€ Found what I need? â†’ STOP (no Perplexity)
â””â”€â”€ Still insufficient? â†’ Continue

Step 3: Evaluate - Is this a high-value use case?
â”œâ”€â”€ NO â†’ Use alternative (codebase-retrieval, training data)
â””â”€â”€ YES â†’ State: "web-search returned [X]. Insufficient because [Y]. Escalating."
          â””â”€â”€ Use appropriate Perplexity tool
```

### API Unavailability

If Perplexity returns 401/403/5xx errors:
1. **Do NOT retry repeatedly** - Wastes time
2. **Fallback to `web-search`** - Often sufficient for targeted queries
3. **Use `web-fetch`** - To get full content from authoritative sources
4. **Inform user** - "Perplexity unavailable, using web search fallback"

## Key Principles

1. **FREE FIRST** - Always try web-search and web-fetch before Perplexity
2. **ALWAYS announce** - User must know when Perplexity is being consulted
3. **Justify escalation** - State what web-search found and why it's insufficient
4. **Track everything** - Stats enable tuning and improvement
5. **Rich prompts** - Better prompts = better results
6. **Broad scope** - Technical AND domain questions are valid
7. **Low threshold** - 2 failures is enough; don't struggle unnecessarily
8. **Cost awareness** - Perplexity costs real money; use only when free tools fail

