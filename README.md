# superpowers-plus

> **Guidelines:** See [CLAUDE.md](./CLAUDE.md) for writing standards.
> **Last Updated:** 2026-02-01

11 skills extending [obra/superpowers](https://github.com/obra/superpowers) for Claude Code, Augment Code, OpenAI Codex CLI, Gemini, and GitHub Copilot.

## What This Does

Detects AI-generated text (300+ patterns, 13 content types), screens resumes for Senior SDE hiring, and enforces code style before commits. See the [Skills Overview](#skills-overview) for the full list.

## Installation

```bash
# Clone this repo
git clone https://github.com/bordenet/superpowers-plus.git
cd superpowers-plus

# Install obra/superpowers (if not present) and all skills
./install.sh
```

The install script:
- Clones obra/superpowers to `~/.codex/superpowers/` if not already installed
- Installs all skills from this repo to `~/.codex/skills/`
- Validates the installation

Use `./install.sh --verbose` for detailed output or `./install.sh --force` to reinstall superpowers.

## Perplexity MCP Integration

The `perplexity-research` skill enables AI assistants to automatically consult Perplexity when stuck.

### Quick Install (New Machine)

```bash
# 1. Clone this repo
git clone https://github.com/bordenet/superpowers-plus.git
cd superpowers-plus

# 2. Install base superpowers and skills
./install.sh

# 3. Configure Perplexity MCP (requires API key)
./setup/mcp-perplexity.sh

# 4. Install the perplexity-research skill
./setup/install-perplexity-skill.sh

# 5. Verify everything works
./setup/verify-perplexity-setup.sh
```

### Automatic Triggers

The skill auto-invokes when:
- **2+ failed attempts** at the same operation
- **Uncertainty/guessing** at an answer
- **Hallucination risk** (unsure about APIs/facts)
- **Outdated knowledge** (post-training topics)

### Manual Override

Say: "Use Perplexity to research X" or "Get unstuck on X"

### Stats Tracking

Stats are tracked in `~/.codex/perplexity-stats.json`:
```bash
cat ~/.codex/perplexity-stats.json | jq .
```

The skill uses a 4-step evaluation loop:
1. **Report** - Summarize Perplexity response
2. **Apply** - Actually use the information
3. **Evaluate** - Judge if it helped (SUCCESS/PARTIAL/FAILURE)
4. **Track** - Record stats only after evaluation

---

## Skills Overview

| Skill | Purpose |
|-------|---------|
| `detecting-ai-slop` | Analyze text and produce bullshit factor scores (0-100) |
| `eliminating-ai-slop` | Rewrite text to remove slop patterns |
| `enforce-style-guide` | Enforce coding standards before commits |
| `incorporating-research` | Incorporate external research into docs (strips artifacts, preserves voice) |
| `perplexity-research` | Auto-invoke Perplexity when stuck (2+ failures, uncertainty) |
| `readme-authoring` | Author and maintain README.md files with best practices and anti-slop enforcement |
| `resume-screening` | Screen Senior SDE candidates against hiring criteria |
| `phone-screen-prep` | Prepare phone screen notes with targeted questions |
| `phone-screen-synthesis` | Generate HIRE/NO-HIRE recommendation with evidence synthesis |
| `reviewing-ai-text` | *(Deprecated)* Use detecting-ai-slop and eliminating-ai-slop instead |
| `security-upgrade` | CVE scanning and dependency upgrade workflow |

---

## Skills: Goals & Outcomes

> **CRITICAL**: These GOALS/OUTCOMES define what each skill MUST accomplish. If a skill fails to achieve its goals, it is useless and should be fixed or deprecated.

### detecting-ai-slop

**Goal:** Identify AI-generated content by detecting telltale patterns (boosters, hedging, clichÃ©s, uniformity) and produce an actionable bullshit factor score.

**Success Criteria:**
- [ ] Score accurately reflects AI-generation likelihood (0-100)
- [ ] Human-written text scores <30 consistently
- [ ] Obvious AI slop scores >60 consistently
- [ ] Breakdown identifies SPECIFIC patterns, not vague categories
- [ ] Stylometric metrics (sentence Ïƒ, TTR, hapax) calculated correctly

**Expected Outcomes:**
1. User pastes text â†’ receives numeric score + pattern breakdown in <5 seconds
2. Score matches human intuition (calibrated via user samples)
3. Actionable fixes: each flagged pattern has a concrete fix suggestion
4. Content-type detection adjusts thresholds appropriately (email vs PRD)

**Failure Modes:**
- âŒ High scores on obviously human text (false positive)
- âŒ Low scores on ChatGPT-generated text (false negative)
- âŒ Vague output: "some issues detected" without specifics
- âŒ Takes >30 seconds to analyze reasonable-length text

**Invoke:** "What's the bullshit factor on this [content type]?"

---

### eliminating-ai-slop

**Goal:** Transform AI-like text into human-quality writing by eliminating detected patterns while preserving meaning and adding specificity.

**Success Criteria:**
- [ ] GVR loop reduces bullshit factor by â‰¥30 points
- [ ] Output passes stylometric thresholds (sentence Ïƒ >3, TTR >0.45)
- [ ] Meaning preservedâ€”no information loss
- [ ] Matches target voice when calibrated
- [ ] Max 3 GVR iterations (prevents infinite loops)

**Expected Outcomes:**
1. Input text with score 70 â†’ output text with score <40
2. User can verify before/after with side-by-side comparison
3. Transparency report explains what changed and why
4. Personal dictionary updates persist across sessions

**Failure Modes:**
- âŒ Output loses critical information
- âŒ Rewritten text sounds generic or loses voice
- âŒ Infinite GVR loops (>3 iterations)
- âŒ No improvement after rewriting

**Invoke:**
- Interactive: "Clean up this email: [text]"
- Automatic: "Write an email to the team about [topic]"
- Calibrate: "Calibrate slop detection with my writing"

---

### enforce-style-guide

**Goal:** Prevent non-compliant code from being committed by enforcing repository-specific and language-specific standards.

**Success Criteria:**
- [ ] Runs automatically before every commit
- [ ] Checks ALL mandatory requirements from style guides
- [ ] Reports violations with file:line references
- [ ] Provides fix suggestions or auto-fixes where possible
- [ ] Re-validates after fixes

**Expected Outcomes:**
1. User attempts commit â†’ skill audits all changed files
2. Violations listed with actionable fix instructions
3. After fixes, re-audit confirms compliance
4. No non-compliant code reaches the repository

**Failure Modes:**
- âŒ Misses obvious violations (false negative)
- âŒ Flags compliant code (false positive)
- âŒ Vague error: "style violation" without location
- âŒ Skipped for some commits (inconsistent enforcement)

**Invoke:** Before ANY commit to ANY repository.

---

### incorporating-research

**Goal:** Integrate external research (Perplexity, web, ChatGPT) into existing documents by stripping citation artifacts and matching document voice.

**Success Criteria:**
- [ ] Citation artifacts (numbers, source sections) removed
- [ ] Document voice/tone preserved after integration
- [ ] Only relevant content extracted (triage works)
- [ ] User confirms scope before any edits
- [ ] No duplicate content introduced

**Expected Outcomes:**
1. User pastes Perplexity output â†’ skill triages relevant vs noise
2. Skill proposes specific insertions with location
3. User confirms â†’ content integrated matching existing style
4. Original document structure preserved

**Failure Modes:**
- âŒ Artifacts remain in final document (e.g., "[1]" citations)
- âŒ Integrated content clashes with document voice
- âŒ Entire paste incorporated without triage
- âŒ Edits made without user confirmation

**Invoke:**
- "Incorporate this Perplexity output into [doc]"
- "Merge this research into the doc"
- "Add this to [file]"

---

### perplexity-research

**Goal:** Automatically invoke Perplexity when the AI assistant is stuck, uncertain, or at risk of hallucinatingâ€”then evaluate if the research actually helped.

**Success Criteria:**
- [ ] Auto-triggers on 2+ failed attempts at same operation
- [ ] Auto-triggers on uncertainty/guessing
- [ ] Uses correct Perplexity tool (ask vs research vs search)
- [ ] 4-step evaluation loop: Report â†’ Apply â†’ Evaluate â†’ Track
- [ ] Stats only recorded AFTER evaluating helpfulness

**Expected Outcomes:**
1. AI hits wall â†’ Perplexity invoked automatically
2. Response summarized, then APPLIED to the problem
3. Outcome evaluated: SUCCESS (fixed it), PARTIAL (helped some), FAILURE (didn't help)
4. Stats tracked with outcome, enabling success rate analysis

**Failure Modes:**
- âŒ Perplexity called but response not applied (wasted API call)
- âŒ Stats recorded before knowing if it helped (inflated success rates)
- âŒ Over-triggers on easy problems (wastes API quota)
- âŒ Never triggers even when clearly stuck

**Invoke:** Automatic, or manual: "Use Perplexity to research X"

---

### resume-screening

**Goal:** Efficiently screen Senior SDE candidates against hiring criteria, flagging concerns and generating targeted phone screen questions.

**Success Criteria:**
- [ ] Work authorization checked FIRST (reject if needs sponsorship)
- [ ] 5+ years qualifying SWE experience verified
- [ ] Contractor patterns detected and flagged
- [ ] Salary alignment validated against cap
- [ ] AI slop detected in resumes/responses
- [ ] Phone screen questions target identified concerns

**Expected Outcomes:**
1. User pastes resume â†’ HIRE/NO HIRE/PROBE decision in <30 seconds
2. Phone screen questions generated for each concern
3. Loop questions generated for deeper probing
4. AI slop flagged if detected in resume or responses

**Failure Modes:**
- âŒ Misses sponsorship requirement (wastes interview time)
- âŒ Passes unqualified candidate (< 5 years SWE)
- âŒ Rejects strong candidate on technicality (over-filtering)
- âŒ Generic questions instead of targeted probes

**Invoke:**
- "Screen at $[X]k cap" + paste resume
- "What's the bullshit factor on this resume?" (slop analysis)

---

### phone-screen-prep

**Goal:** Create phone screen notes files with targeted questions based on screening concerns, ready for the interviewer.

**Success Criteria:**
- [ ] Template copied and placeholders replaced
- [ ] Targeted questions added for each screening concern
- [ ] AI slop probing questions added when bullshit factor >50
- [ ] File named correctly (FirstName_LastName__YYYY-MM-DD.md)
- [ ] Links populated (Paylocity, GitHub, LinkedIn)

**Expected Outcomes:**
1. User requests prep â†’ file created in correct location
2. Concerns from resume screening â†’ targeted questions in file
3. High AI slop â†’ explicit authenticity probing questions added
4. File ready for interviewer to use during call

**Failure Modes:**
- âŒ Wrong file location or naming
- âŒ Missing targeted questions (generic template only)
- âŒ AI slop concerns not translated to probe questions
- âŒ Links/placeholders not populated

**Invoke:** "Prep phone screen for [Name]"

---

### security-upgrade

**Goal:** Systematically scan for CVEs, upgrade vulnerable dependencies, and verify fixes across all supported package managers.

**Success Criteria:**
- [ ] All package managers scanned (npm, Go, Python, Rust, Flutter)
- [ ] CVEs identified with severity and fix versions
- [ ] Upgrades applied one at a time with testing
- [ ] Breaking changes detected and addressed
- [ ] Post-upgrade verification confirms fixes

**Expected Outcomes:**
1. User invokes â†’ full vulnerability scan across all ecosystems
2. CVEs listed with severity, affected package, fix version
3. Each upgrade tested before proceeding to next
4. Final verification shows 0 vulnerabilities (or documented exceptions)

**Failure Modes:**
- âŒ Misses vulnerabilities (incomplete scan)
- âŒ Bulk upgrades break the build
- âŒ Upgrades applied without testing
- âŒ Says "done" but vulnerabilities remain

**Invoke:** "Scan for CVEs" or "Upgrade dependencies"

---

### reviewing-ai-text *(DEPRECATED)*

**Goal:** N/A â€” Use `detecting-ai-slop` and `eliminating-ai-slop` instead.

This skill is deprecated. It has been superseded by the more capable slop detection and elimination skills which provide:
- Better pattern detection (300+ patterns vs original ~50)
- GVR loop for automatic rewriting
- Stylometric analysis
- Cross-machine dictionary sync

## Golden Agents Framework

The `guidance/` directory generates Agents.md files for new projects, covering superpowers bootstrap, anti-slop rules, and language-specific guidance (Go, Python, JavaScript, Shell, Dart).

### Quick Start

```bash
# Generate Agents.md for a Go CLI project
./guidance/seed.sh --language=go --type=cli-tools --path=./my-project

# Generate for a Flutter mobile app
./guidance/seed.sh --language=dart-flutter --type=mobile-apps --path=./my-app

# Preview without writing
./guidance/seed.sh --language=javascript --type=web-apps --dry-run
```

### What's Included

| Category | Modules |
|----------|---------|
| **Core** | superpowers bootstrap, communication standards, anti-slop rules |
| **Workflows** | deployment, testing, security, session-resumption, build-hygiene |
| **Languages** | Go, Python, JavaScript, Shell, Dart/Flutter |
| **Project Types** | CLI tools, web apps, genesis tools, mobile apps |

Generated files are self-contained (no external references) and typically 300-800 lines depending on selected options.

See [guidance/README.md](./guidance/README.md) for full documentation.

---

## Cross-Machine Sync

The `slop-sync` script synchronizes your slop dictionary across machines via GitHub:

```bash
# Initialize (first time only)
./slop-sync init

# Upload dictionary after changes
./slop-sync push

# Download latest on another machine
./slop-sync pull

# Check sync status
./slop-sync status
```

Uses Last Write Wins conflict resolution based on timestamps.

## Directory Structure

```
superpowers-plus/
â”œâ”€â”€ Agents.md                       # Primary AI guidance
â”œâ”€â”€ CLAUDE.md                       # Redirect â†’ Agents.md
â”œâ”€â”€ CODEX.md                        # Redirect â†’ Agents.md
â”œâ”€â”€ GEMINI.md                       # Redirect â†’ Agents.md
â”œâ”€â”€ COPILOT.md                      # Redirect â†’ Agents.md
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ TODO.md                         # Task tracking
â”œâ”€â”€ LICENSE
â”œâ”€â”€ install.sh                      # Install superpowers and skills
â”œâ”€â”€ slop-sync                       # Cross-machine dictionary sync script
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ Vision_PRD.md               # High-level vision and requirements
â”‚   â”œâ”€â”€ PRD_detecting-ai-slop.md    # Detector skill requirements
â”‚   â”œâ”€â”€ PRD_eliminating-ai-slop.md  # Eliminator skill requirements
â”‚   â”œâ”€â”€ DESIGN.md                   # Technical design
â”‚   â””â”€â”€ TEST_PLAN.md                # Test plan (80+ test cases)
â”œâ”€â”€ guidance/                       # ğŸ†• Golden Agents Framework
â”‚   â”œâ”€â”€ Agents.md                   # AI guidance for this directory
â”‚   â”œâ”€â”€ CLAUDE.md                   # Redirect â†’ Agents.md
â”‚   â”œâ”€â”€ CODEX.md                    # Redirect â†’ Agents.md
â”‚   â”œâ”€â”€ GEMINI.md                   # Redirect â†’ Agents.md
â”‚   â”œâ”€â”€ COPILOT.md                  # Redirect â†’ Agents.md
â”‚   â”œâ”€â”€ README.md                   # Framework documentation
â”‚   â”œâ”€â”€ seed.sh                     # Generator script
â”‚   â”œâ”€â”€ TEMPLATE-minimal.md         # Minimal template (~100 lines)
â”‚   â”œâ”€â”€ TEMPLATE-full.md            # Full template with placeholders
â”‚   â”œâ”€â”€ core/                       # Core guidance (always included)
â”‚   â”œâ”€â”€ workflows/                  # Development workflow guidance
â”‚   â”œâ”€â”€ languages/                  # Language-specific guidance
â”‚   â””â”€â”€ project-types/              # Project type guidance
â””â”€â”€ skills/
    â”œâ”€â”€ detecting-ai-slop/          # Analysis and scoring (300+ patterns)
    â”‚   â””â”€â”€ SKILL.md
    â”œâ”€â”€ eliminating-ai-slop/        # Rewriting and prevention (GVR loop)
    â”‚   â””â”€â”€ SKILL.md
    â”œâ”€â”€ enforce-style-guide/
    â”‚   â””â”€â”€ SKILL.md
    â”œâ”€â”€ incorporating-research/     # Incorporate external research (strips artifacts)
    â”‚   â””â”€â”€ SKILL.md
    â”œâ”€â”€ perplexity-research/        # Auto-invoke Perplexity when stuck
    â”‚   â””â”€â”€ SKILL.md
    â”œâ”€â”€ readme-authoring/           # README.md best practices + anti-slop
    â”‚   â””â”€â”€ SKILL.md
    â”œâ”€â”€ resume-screening/           # Integrates with detecting-ai-slop
    â”‚   â”œâ”€â”€ SKILL.md
    â”‚   â””â”€â”€ README.md
    â”œâ”€â”€ phone-screen-prep/          # Adds AI slop probing questions
    â”‚   â”œâ”€â”€ SKILL.md
    â”‚   â””â”€â”€ README.md
    â”œâ”€â”€ phone-screen-synthesis/     # HIRE/NO-HIRE recommendation
    â”‚   â”œâ”€â”€ SKILL.md
    â”‚   â””â”€â”€ README.md
    â”œâ”€â”€ security-upgrade/           # CVE scanning and upgrades
    â”‚   â””â”€â”€ SKILL.md
    â””â”€â”€ reviewing-ai-text/          # Deprecated
        â””â”€â”€ SKILL.md
```

## Creating New Skills

1. Create a new directory under `skills/`
2. Add `SKILL.md` with frontmatter (name, description)
3. Run `./install.sh` to deploy
4. Test with `~/.codex/superpowers/.codex/superpowers-codex use-skill <skill-name>`

See [superpowers:writing-skills](https://github.com/obra/superpowers) for skill authoring guidelines.

## Documentation

| Document | Purpose |
|----------|---------|
| [Vision_PRD.md](./docs/Vision_PRD.md) | High-level vision and requirements |
| [PRD_detecting-ai-slop.md](./docs/PRD_detecting-ai-slop.md) | Detector requirements (13 content types) |
| [PRD_eliminating-ai-slop.md](./docs/PRD_eliminating-ai-slop.md) | Eliminator requirements (11 rewriting strategies) |
| [DESIGN.md](./docs/DESIGN.md) | Technical architecture |
| [TEST_PLAN.md](./docs/TEST_PLAN.md) | Test plan (80+ test cases) |

## Author

Matt J Bordenet (@bordenet)
